import streamlit as st
import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. ì„¤ì • ë¡œë“œ
load_dotenv()
st.set_page_config(page_title="RFP ê²€ìƒ‰ê¸°", page_icon="ğŸ”")
DB_PATH = "./rfp_database"

# 2. ì œëª© ë° ì„¤ëª…
st.title("ğŸ¤– AI RFP ë¶„ì„ê¸°")
st.caption("ë¬¸ì„œ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ê³ , ì°¸ê³ í•œ ì›ë¬¸ê¹Œì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.")

# --- [ì¶”ê°€] ì‚¬ì´ë“œë°”: DBì—ì„œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì™€ì„œ ë³´ì—¬ì£¼ê¸° ---
st.sidebar.header("ğŸ“‚ ë¬¸ì„œ ì„ íƒ")

# DBì— ì–´ë–¤ íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
def get_file_list():
    if os.path.exists(DB_PATH):
        # DBë¥¼ ì ì‹œ ì—´ì–´ì„œ ë©”íƒ€ë°ì´í„°ë§Œ í™•ì¸
        _embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        _db = Chroma(persist_directory=DB_PATH, embedding_function=_embeddings)
        data = _db.get()
        # ì €ì¥ëœ ëª¨ë“  ë¬¸ì„œì˜ ê²½ë¡œ(source)ë¥¼ ê°€ì ¸ì™€ì„œ ì¤‘ë³µ ì œê±°
        return list(set([m['source'] for m in data['metadatas']]))
    return []

# íŒŒì¼ ëª©ë¡ ë¡œë“œ
all_files = get_file_list()
# {íŒŒì¼ëª…: ì „ì²´ê²½ë¡œ} í˜•íƒœë¡œ ì§ê¿ ë§Œë“¤ê¸°
file_map = {os.path.basename(f): f for f in all_files}

# ì„ íƒ ë°•ìŠ¤ ë§Œë“¤ê¸°
selected_file = st.sidebar.selectbox(
    "ê²€ìƒ‰í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
    ["ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰"] + list(file_map.keys())
)

# 3. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! RFP ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}]

# 4. ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# 5. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # --- [RAG ë¡œì§ ì‹œì‘] ---
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    if os.path.exists(DB_PATH):
        vectordb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
        
        # --- [ì¶”ê°€] ì„ íƒí•œ íŒŒì¼ë§Œ ë³´ê²Œ í•˜ëŠ” í•„í„°ë§ ë¡œì§ ---
        search_kwargs = {"k": 7} # ê¸°ë³¸ì€ 3ê°œ ê²€ìƒ‰
        
        if selected_file != "ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰":
            # ì‚¬ìš©ìê°€ íŒŒì¼ì„ ì„ íƒí–ˆë‹¤ë©´, ê·¸ íŒŒì¼ ê²½ë¡œ(source)ë‘ ë˜‘ê°™ì€ ê²ƒë§Œ ì°¾ìœ¼ë¼ê³  ëª…ë ¹
            target_path = file_map[selected_file]
            search_kwargs["filter"] = {"source": target_path}
            st.toast(f"ğŸ”’ '{selected_file}' ë¬¸ì„œ ì•ˆì—ì„œë§Œ ì°¾ìŠµë‹ˆë‹¤.")
            
        # í•„í„°ê°€ ì ìš©ëœ ê²€ìƒ‰ê¸° ìƒì„±
        retriever = vectordb.as_retriever(search_kwargs=search_kwargs) 
        
        # ... (ì´ ì•„ë˜ retrieved_docs = retriever.invoke(prompt) ë¶€í„°ëŠ” ê·¸ëŒ€ë¡œ ë‘ ) ...
        # [ë‹¨ê³„ 1] ë¬¸ì„œë¥¼ ë¨¼ì € ê²€ìƒ‰í•´ ì˜µë‹ˆë‹¤ (Retriever ì‹¤í–‰)
        with st.spinner("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ëŠ” ì¤‘..."):
            retrieved_docs = retriever.invoke(prompt)
        
        # ë¬¸ì„œ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
        def format_docs(docs):
            return "\n\n".join([d.page_content for d in docs])
            
        context_text = format_docs(retrieved_docs)

        # [ë‹¨ê³„ 2] ì´ì „ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        chat_history_str = ""
        for msg in st.session_state.messages[:-1]:
            role = "Human" if msg["role"] == "user" else "AI"
            chat_history_str += f"{role}: {msg['content']}\n"

        # [ë‹¨ê³„ 3] í”„ë¡¬í”„íŠ¸ ë° ë‹µë³€ ìƒì„±
        template = """
        ë‹¹ì‹ ì€ ì œì•ˆìš”ì²­ì„œ(RFP) ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì•„ë˜ [ì°¸ê³  ë¬¸ì„œ]ë§Œì„ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
        ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì— ë‚˜ì™€ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ì†”ì§íˆ ë§í•˜ì„¸ìš”.
        
        [ì´ì „ ëŒ€í™” ê¸°ë¡]:
        {chat_history}

        [ì°¸ê³  ë¬¸ì„œ]: 
        {context}
        
        ì§ˆë¬¸: {question}
        """
        rag_prompt = ChatPromptTemplate.from_template(template)
        
        # ëª¨ë¸ ì„¤ì •
        model = ChatOpenAI(model="gpt-5-mini", temperature=0)
        
        chain = rag_prompt | model | StrOutputParser()
        
        with st.chat_message("assistant"):
            # ë‹µë³€ ì¶œë ¥
            with st.spinner("ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„± ì¤‘..."):
                response = chain.invoke({
                    "context": context_text,
                    "question": prompt,
                    "chat_history": chat_history_str
                })
                st.write(response)
            
            # [í•µì‹¬ ê¸°ëŠ¥ ì¶”ê°€] ì°¸ê³ í•œ ë¬¸ì„œ ì›ë¬¸ ë³´ì—¬ì£¼ê¸°
            with st.expander("ğŸ“š ì°¸ê³ í•œ ë¬¸ì„œ ì›ë¬¸ ë³´ê¸° (í´ë¦­)"):
                for i, doc in enumerate(retrieved_docs):
                    source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ').split('/')[-1]
                    st.markdown(f"**ğŸ“– ì°¸ê³  ë¬¸ì„œ {i+1}: {source}**")
                    st.info(doc.page_content[:300] + "...") # ë„ˆë¬´ ê¸°ë‹ˆê¹Œ 300ìë§Œ ë¯¸ë¦¬ë³´ê¸°
        
        # ê¸°ë¡ ì €ì¥
        st.session_state.messages.append({"role": "assistant", "content": response})
        
    else:
        st.error("ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì„ë² ë”©í•´ì£¼ì„¸ìš”.")

# ê°€ìƒí™˜ê²½ ì¼œê¸°: source ~/myenv/bin/activate
# ì‹¤í–‰ë¬¸(í„°ë¯¸ë„ì— ì…ë ¥): streamlit run entrypoint/app.py