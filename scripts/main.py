import os
import glob
import re
import olefile
import zlib
import struct
import fitz  # PyMuPDF
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

# --- [ì„¤ì •] ë§Œë“¤ DB ëª©ë¡ ---
TARGET_DBS = [
    {"name": "bge-m3",  "path": "./rfp_database_bge"},
    {"name": "kure-v1", "path": "./rfp_database_kure"}
]

load_dotenv()
DATA_DIR = "./data/01-raw"

# --- [ì „ì²˜ë¦¬ í•¨ìˆ˜] ---
def clean_text(text):
    """
    ë…¸ì´ì¦ˆ ì œê±°: 
    1.  (Unit Separator), ìˆ˜ì§ íƒ­, í¼ í”¼ë“œ ë“± í™”ë©´ì— ì´ìƒí•˜ê²Œ ì°íˆëŠ” ì œì–´ ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    2. í•„ìš”í•œ í•œê¸€/ì˜ì–´/ìˆ«ì/ê¸°í˜¸ëŠ” ê·¸ëŒ€ë¡œ ì‚´ë¦½ë‹ˆë‹¤.
    """
    # íŠ¹ìˆ˜ë¬¸ì(=\x1f) ë° ì œì–´ ë¬¸ì ê°•ë ¥ ì œê±°
    text = text.replace("\x1f", " ").replace("\x0b", " ").replace("\x0c", " ")
    text = re.sub(r'[\x00-\x08\x0e-\x1f\x7f]', ' ', text)
    
    # ê¸°ì¡´ í—ˆìš© íŒ¨í„´ ìœ ì§€
    pattern = r"[^ê°€-í£a-zA-Z0-9\s\.,\-\(\)\[\]\%\~\'\"Â·]"
    text = re.sub(pattern, " ", text)
    
    # ê³µë°± ì •ë¦¬
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()

# --- [HWP ì¶”ì¶œ í•¨ìˆ˜] ---
def get_hwp_text(filename):
    try:
        f = olefile.OleFileIO(filename)
        dirs = f.listdir()
        
        if not any(d[0] == "BodyText" for d in dirs): return ""
        
        nums = []
        for d in dirs:
            if d[0] == "BodyText":
                try: nums.append(int(d[1].replace("Section", "")))
                except: pass
        nums.sort()
        
        header = f.openstream("FileHeader")
        is_compressed = (header.read()[36] & 1) == 1
        
        text = ""
        for i in nums:
            b_data = f.openstream(f"BodyText/Section{i}").read()
            if is_compressed: b_data = zlib.decompress(b_data, -15)
            
            i = 0
            while i < len(b_data):
                header = struct.unpack_from("<I", b_data, i)[0]
                rec_len = (header >> 20) & 0xfff
                if (header & 0x3ff) == 67:
                    rec_payload = b_data[i+4:i+4+rec_len]
                    text += rec_payload.decode('utf-16-le', errors='ignore') + "\n"
                i += 4 + rec_len
        return clean_text(text)
    except Exception as e:
        print(f"âš ï¸ HWP ì½ê¸° ì—ëŸ¬({os.path.basename(filename)}): {e}")
        return ""

# --- [PDF ì¶”ì¶œ í•¨ìˆ˜] ---
def get_pdf_text(filename):
    text = ""
    try:
        doc = fitz.open(filename)
        for page in doc:
            text += page.get_text(sort=True) + "\n"
        doc.close()
        return clean_text(text)
    except:
        return ""

# --- [ë©”ì¸ ì‹¤í–‰] ---
if __name__ == "__main__":
    print(f"ğŸš€ [ìµœì¢… DB ìƒì„±ê¸°] ë°ì´í„° ë¡œë”© ì‹œì‘: {DATA_DIR}")
    
    docs = []
    files = glob.glob(os.path.join(DATA_DIR, "*.*"))

    for f in files:
        filename = os.path.basename(f)
        ext = f.split('.')[-1].lower()
        content = ""
        
        if ext == 'hwp':
            content = get_hwp_text(f)
            if "ë²¤ì²˜" in filename:
                print(f"ğŸ‘€ [í™•ì¸] {filename} ì½ê¸° ì„±ê³µ! (ê¸¸ì´: {len(content)})")
                if "352,000,000" in content:
                    print("   -> âœ… í•µì‹¬ ë°ì´í„°(352,000,000) í¬í•¨ë¨!")
        elif ext == 'pdf':
            content = get_pdf_text(f)
        else:
            continue
            
        if content:
            # íŒŒì¼ëª…ë§Œ ì €ì¥ (í•„í„°ë§ ì˜¤ë¥˜ ë°©ì§€)
            docs.append(Document(page_content=content, metadata={"source": filename}))

    if not docs:
        print("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    print(f"\nì´ {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ. ì²­í‚¹ ì‹œì‘...")

    # [í•µì‹¬ ìˆ˜ì •] kure-v1ì„ ìœ„í•´ 600ìë¡œ ì•ˆì „í•˜ê²Œ ì¶•ì†Œ! (Overlap 100)
    # ì´ì œ kure-v1ì´ ì†Œí™”ë¶ˆëŸ‰ì— ê±¸ë¦¬ì§€ ì•Šê³  ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ê¼¼ê¼¼íˆ ì”¹ì–´ë¨¹ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600, 
        chunk_overlap=100,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = splitter.split_documents(docs)

    print("\n-------------------------------------------------")
    for db_info in TARGET_DBS:
        model_name = db_info["name"]
        db_path = db_info["path"]
        
        print(f"ğŸ”¥ {model_name} DB ìƒì„± ì¤‘... ({db_path})")
        try:
            embeddings = OllamaEmbeddings(model=model_name)
            import shutil
            if os.path.exists(db_path):
                shutil.rmtree(db_path)
            
            Chroma.from_documents(chunks, embeddings, persist_directory=db_path)
            print(f"âœ… {model_name} ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ì‹¤íŒ¨: {e}")
        print("-------------------------------------------------")
          
# ê¸°ì¡´ DB ì‚­ì œ
# í„°ë¯¸ë„ì—: rm -rf rfp_database_bge rfp_database_kure
# ê°€ìƒí™˜ê²½ ì¼œê¸°: source .venv/bin/activate  