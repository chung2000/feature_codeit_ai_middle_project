import streamlit as st
import os
import sys
import time

# [SQLite í˜¸í™˜ì„± íŒ¨ì¹˜]
try:
    import pysqlite3
    if not hasattr(pysqlite3, "sqlite_version_info"):
        pysqlite3.sqlite_version_info = (3, 35, 0)
    sys.modules["sqlite3"] = pysqlite3
except ImportError:
    pass

# ì„¤ì • íŒŒì¼ ë¡œë”
try:
    from src.common.config import config
except:
    config = {}

from src.indexing.vector_store import VectorStoreWrapper
from src.generation.rag import RAGChain

st.set_page_config(page_title="RAG ChatBot", page_icon="ğŸ¤–", layout="wide")

# --- [í•µì‹¬] DB ë¡œë”© í•¨ìˆ˜ ---
@st.cache_resource
def load_vector_store(embedding_model_name):
    """
    ì„ íƒëœ ì„ë² ë”© ëª¨ë¸("bge-m3" or "kure-v1")ì— ë§ëŠ” DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    db_paths = {
        "bge-m3": "./rfp_database_bge",
        "kure-v1": "./rfp_database_kure"
    }
    target_path = db_paths.get(embedding_model_name)
    
    temp_config = config.copy()
    temp_config["vector_db_path"] = target_path
    temp_config["embedding_model"] = embedding_model_name
    
    wrapper = VectorStoreWrapper(temp_config)
    wrapper.initialize()
    return wrapper

def reset_selected_docs():
    st.session_state.selected_docs = []  # ë¬¸ì„œ ì„ íƒ ì´ˆê¸°í™”
    st.toast("ğŸ”„ ê²€ìƒ‰ ì—”ì§„ì´ ë³€ê²½ë˜ì–´ ë¬¸ì„œ ì„ íƒì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="âœ¨")

# --- ì‚¬ì´ë“œë°” UI (ìƒë‹¨ ì„¤ì •) ---
with st.sidebar:
    st.header("ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì •")
    
    # 1. ì„ë² ë”© ëª¨ë¸ (DB) ì„ íƒ
    st.subheader("ğŸ§  ê²€ìƒ‰ ì—”ì§„ (Embedding)")
    selected_embedding = st.radio(
        "ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸",
        ["bge-m3", "kure-v1"],
        index=0,
        help="bge-m3: ë‹¤êµ­ì–´ ë²”ìš© (ê¸°ë³¸)\nkure-v1: í•œêµ­ì–´ íŠ¹í™” (ê²½ëŸ‰)",
        on_change=reset_selected_docs
    )
    
    st.divider()
    
    # 2. ë‹µë³€ ëª¨ë¸ (LLM) ì„ íƒ
    st.subheader("ğŸ¤– ë‹µë³€ AI (LLM)")
    selected_llm = st.selectbox(
        "ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸",
        ["gemma3:12b", "llama3.1"],
        index=0,
        help="Gemma3: ì •í™•ë„ ì¤‘ì‹œ\nLlama3.1: ì†ë„ ì¤‘ì‹œ"
    )

    st.divider()


# --- ì‹œìŠ¤í…œ ë¡œë”© ë° í•˜ë‹¨ ì‚¬ì´ë“œë°” UI ---
try:
    vector_store_wrapper = load_vector_store(selected_embedding)
    all_docs = vector_store_wrapper.get_all_documents()
    
    with st.sidebar:
        st.subheader("ğŸ“‚ ë¬¸ì„œ í•„í„°")
        
        # session_stateì— ê°’ì´ ì—†ìœ¼ë©´ ë¯¸ë¦¬ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”í•´ì¤ë‹ˆë‹¤.
        if "selected_docs" not in st.session_state:
            st.session_state.selected_docs = []
            
        selected_docs = st.multiselect(
            "ë¶„ì„ ëŒ€ìƒ ë¬¸ì„œ",
            options=all_docs,
            key="selected_docs"
        )
        
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
            
        # ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´ ê³ ì • 
        st.markdown("---")
        st.success(
            f"ğŸ”‹ **Current System**\n\n"
            f"ğŸ§  Emb: **{selected_embedding}**\n"
            f"ğŸ¤– LLM: **{selected_llm}**"
        )

except Exception as e:
    st.error(f"ğŸš¨ DB ë¡œë”© ì‹¤íŒ¨! í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\nì—ëŸ¬: {e}")
    st.stop()


# --- RAG ì²´ì¸ êµ¬ì„± ---
current_settings = f"{selected_embedding}_{selected_llm}"

if "rag_chain" not in st.session_state or st.session_state.get("current_settings") != current_settings:
    with st.spinner(f"âš™ï¸ ì—”ì§„ êµì²´ ì¤‘... (DB: {selected_embedding} / LLM: {selected_llm})"):
        st.session_state.rag_chain = RAGChain(
            config, 
            vector_store_wrapper, 
            model_name=selected_llm
        )
        st.session_state.current_settings = current_settings
    st.toast(f"âœ… ì‹œìŠ¤í…œ ì„¤ì • ë³€ê²½ ì™„ë£Œ!", icon="ğŸ”„")


# --- ë©”ì¸ í™”ë©´ ---

if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ê¸°ë¡ì´ ì—†ì„ ë•Œë§Œ(0ê°œì¼ ë•Œë§Œ) íƒ€ì´í‹€ê³¼ ê°€ì´ë“œ í‘œì‹œ
if len(st.session_state.messages) == 0:
    st.title("ğŸ¤– AI RFP ë¶„ì„ê¸°")
    
    # ì‚¬ìš© ê°€ì´ë“œë¼ì¸
    st.info("""
    **ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤! ì´ë ‡ê²Œ ì‚¬ìš©í•´ ë³´ì„¸ìš”:**
    
    1. ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ [ğŸ“‚ ë¬¸ì„œ í•„í„°]ë¥¼ ëˆŒëŸ¬ ë¶„ì„í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”.
    2. ì•„ë˜ ì…ë ¥ì°½ì— ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”.
    
    > **(ì˜ˆì‹œ)** *"ì´ ì‚¬ì—…ì˜ ì˜ˆì‚°ì€ ì–¼ë§ˆì•¼?", "ì œì•ˆì„œ ì œì¶œ ë§ˆê°ì¼ì€ ì–¸ì œì•¼?"*
    """)

# ëŒ€í™” ê¸°ë¡ ì¶œë ¥
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if "latency" in msg:
            st.caption(f"â±ï¸ ì†Œìš” ì‹œê°„: {msg['latency']:.2f}ì´ˆ")
        if "sources" in msg and msg["sources"]:
            with st.expander("ğŸ“š ë¶„ì„ì— ì‚¬ìš©ëœ ë¬¸ì„œ"):
                for src in msg["sources"]:
                    st.markdown(f"- **{src['source']}**: {src['content'][:100]}...")

# ì§ˆë¬¸ ì²˜ë¦¬
if prompt := st.chat_input("AI RFP ë¶„ì„ê¸°ì—ê²Œ ë¬¼ì–´ë³´ê¸°"):
    # ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ëŠ” ìˆœê°„, messages ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ë˜ë¯€ë¡œ
    # ë‹¤ìŒ ë¦¬ëŸ°(Rerun) ë•ŒëŠ” ìœ„ìª½ì˜ íƒ€ì´í‹€ê³¼ ê°€ì´ë“œê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤.
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ğŸ” ë¶„ì„ ì¤‘..."):
            start_time = time.time()
            
            answer, docs = st.session_state.rag_chain.generate_answer(prompt, selected_docs)
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            st.markdown(answer)
            st.caption(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
            
            sources = []
            if docs:
                sources = [{"source": os.path.basename(d.metadata.get('source', 'Unknown')), "content": d.page_content} for d in docs]
                with st.expander("ğŸ“š ë¶„ì„ì— ì‚¬ìš©ëœ ë¬¸ì„œ"):
                    for s in sources:
                        st.markdown(f"- **{s['source']}**: {s['content'][:200]}...")

            st.session_state.messages.append({
                "role": "assistant",
                "content": answer,
                "sources": sources,
                "latency": elapsed_time
            })
